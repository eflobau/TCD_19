{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 7. Probability distributions\n",
    "\n",
    "\n",
    "Created by Emanuel Flores-Bautista 2019.  All content contained in this notebook is licensed under a [Creative Commons License 4.0](https://creativecommons.org/licenses/by/4.0/). The code is licensed under a [MIT license](https://opensource.org/licenses/MIT). This notebook was based on the BeBi103 class from the amazing Justin Bois. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:44.845341Z",
     "start_time": "2019-04-26T16:44:42.467074-05:00"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import TCD19_utils as TCD\n",
    "\n",
    "TCD.set_plotting_style_2()\n",
    "\n",
    "#Magic command to enable plotting inside notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#Magic command to enable svg format in plots\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A necessary rant on probability and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T21:36:50.766003Z",
     "start_time": "2019-04-11T16:36:50.749446-05:00"
    }
   },
   "source": [
    "### Why probability ?\n",
    "\n",
    "Data science is inherently a statistical subject. When analyzing large amounts of data we need to know how randomness could affect our observations and how sure can we be about a certain conclusion given our data. \n",
    "\n",
    "One could also argue that an important subject to learn from would be linear algebra as it is the basis for most of the manipulations that are working under the hood in the machine learning models we will see in the next module. While this is true, this workshop is not focused solely on machine learning, but on data science as a whole. Therefore, I think that a proper refreshing on probability and statistics is necessary for a better understanding of the types of things we can do with data analysis. \n",
    "\n",
    "Probability can indeed be a very hard subject because of the inherent mathematical and conceptual complexity behind it. However, we will take a very practical approach and see how we can use the tools from probability and statistics to get a better sense of our data, through the abstraction into certain \"summary statistics\" and to develop an intuition to understand the models we'll use in the next module and to  create more complex mathematical models in the future.\n",
    "\n",
    "It is because of this complexity that it was hard for me to even design this module as I'm trying to re-learn probability and statistics to delve into bayesian inference, information theory, statitstical physics and stochastic modeling of genetic circuits. \n",
    "\n",
    "### Distinction between bayesian and frequentist statistics. \n",
    "\n",
    "Probabilities take values between 0 and 1, where 0 means impossibility and 1 means certainty. However, there are two ways of thinking of probabilities.  \n",
    "\n",
    "Frequentists : Think of how classical stats courses are taught. The frequentist conception of probability $P(A)$of an event A is \n",
    "\n",
    "represents a long-run frequency over a\n",
    "large number of identical repetitions of an experiment.\n",
    "\n",
    "\\begin{align}\n",
    "P(A) = \\frac{ \\text{number of outcomes that favour event A}}{\\text{total number of outcomes}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Bayesian: bayesian stats in the other hand, \"guesses\" a probability in terms of our confidence that a given event might happen, and updates that guess with the more data. \n",
    "\n",
    "\n",
    "Bayesian inference allows us to have an intuition like the following image: \n",
    "\n",
    "While bayesian inference is beyond the scope of this course, you can readily see that bayesian inference is the more \"common\" thing for us humans to think of probabilities. We tend to think of a certain event given the information we know. This beautiful visualization captions our way of thinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:44.865239Z",
     "start_time": "2019-04-26T16:44:44.849604-05:00"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url= \"https://raw.githubusercontent.com/zonination/perceptions/master/joy1.png\", \n",
    "      width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this rant we can now start talking about some important concepts in probability and the stories behind different distributions. Let's begin...\n",
    "\n",
    "If you want to know more about the difference between frequentism and bayesianism read this great [blog post from Jake Van der Plas](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's be non-binary/ stop using p-values\n",
    "\n",
    "A P-value by definition is the probability of observing the test statistic being at least as extreme as what was measured if the null hypothesis is true. We'll see how to calculate p-values using simulations like hacker statisticians, we'll use our computers to simulate the adquisition of new samples given a small dataset to do so. \n",
    "\n",
    "Do not use p-values for a binary classification of statistical significance, believe me. *share papers on p-values  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key concepts\n",
    "\n",
    "Random variable: a variable whose possible values are numerical of an experiment. There are two types of random variables.\n",
    "\n",
    "* Discrete: takes integer values or `int`s such as 1, 2, 3 ...\n",
    "* Continue: takes decimal values or `float`s such as 28763.23\n",
    "\n",
    "* Probability mass function (PMF): describes the probability of a discrete variable obtaining value  $x$ . The variable  $x$  takes on discrete values, so the normalization condition is\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{x}{f(x)} = 1\n",
    "\\end{align}\n",
    "\n",
    "* Probability density function (PDF): A probability density function (PDF), which we shall call  $f(x)$ , is defined such that the probability that a continuous variable  $x$  is  $a \\le x \\le b$  is\n",
    "\n",
    "\\begin{align}\n",
    "\\int_a^b f(x)\\mathrm{d}x.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We all need a moment\n",
    "\n",
    "The moments from probability distributions are often used to calculate summary statistics. We'll only use the first two moments in this tutorial and we'll use their plug-in estimates. The first two moments are the mean and the variance.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{sample mean} = \\bar{X} = \\frac{\\sum_{i}^{n } \\, X_{i} \\,}{n}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\text{sample variance} = \\sigma^{2} = \\frac{\\sum_{i=1}^{N} (x_{i} - \\mu)^2}{N}\n",
    "\\end{align}\n",
    "\n",
    "## Sampling and randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability distributions \n",
    "\n",
    "### Bernoulli trials\n",
    "\n",
    "### Bernoulli distribution\n",
    "\n",
    "* **Story.** A single trial with either a success (`k = True`) or\n",
    "failure (`k=False`) is performed. Such a trial is called a *Bernoulli trial*.  The Bernoulli distribution defines the probability of getting each outcome.\n",
    "\n",
    "* **Parameter.** The Bernoulli distribution is parametrized by a single value, $p$, the probability that the trial is successful. These trials are called *Bernoulli trials*.\n",
    "\n",
    "* **Example.**  The simplest example of a Bernoulli trial is a coin flip, where either side is asigned as a success. However, any event whose value has a binary output can be thought of as a Bernoulli trial. Imagine going to a bar in a foreign country and asking for an unkown beer, whether you'll like the beer or not can be thought of a Bernoulli trial. In Germany or Belgium you could think that the probability of success is high, and so forth.\n",
    "\n",
    "* **Probability mass function.**\n",
    "\n",
    "\\begin{align}\n",
    "f(k;p) = \\left\\{ \\begin{array}{ccc}\n",
    "1-p & & k = 0 \\\\[0.5em]\n",
    "p & & k = 1 \\\\[0.5em]\n",
    "0 & & \\text{otherwise.}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a simple simulation of a coin flip in order to get an intuition of simulating Bernoulli trials in python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:45.080959Z",
     "start_time": "2019-04-26T16:44:44.870461-05:00"
    }
   },
   "outputs": [],
   "source": [
    "# Number of flips\n",
    "n_flips = 10000\n",
    "\n",
    "# List to store the experiment's output\n",
    "output = []\n",
    "\n",
    "# simulate the flips\n",
    "for i in range(0, n_flips):\n",
    "    \n",
    "    # generate random sample from either 1 or 0 \n",
    "    rand = np.random.choice([0,1])\n",
    "    \n",
    "    # Set an '치guila' to values equal to 1\n",
    "    if rand == 1:\n",
    "        \n",
    "        output.append('치guila')\n",
    "        \n",
    "    # Set a 'sol' to values equal to 0\n",
    "    else:\n",
    "        output.append('sol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:45.102232Z",
     "start_time": "2019-04-26T16:44:45.089145-05:00"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of 치guilas: ', output.count('치guila'))\n",
    "print('Number of soles: ', output.count('sol'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T20:38:28.721267Z",
     "start_time": "2019-04-05T14:38:28.711276-06:00"
    }
   },
   "source": [
    "### Geometric distribution\n",
    "\n",
    "* **Story.** We perform a series of Bernoulli trials until we\n",
    "get a success.  We have $k$ failures before the success.\n",
    "\n",
    "* **Parameter.** The Geometric distribution is parametrized by a\n",
    "single value, $p$, the probability that the Bernoulli trial is\n",
    "successful.\n",
    "\n",
    "* **Example.** Imagine you're shooting balls in a bowling alley. The number of $k$ shots you have to take before you get a strike is geometrically distributed. \n",
    "\n",
    "* **Probability mass function.**\n",
    "\n",
    "\\begin{align}\n",
    "f(k;p) = (1-p)^k p.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "* **Notes.** The Geometric distribution is only defined for non-negative\n",
    "integer $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:45.118710Z",
     "start_time": "2019-04-26T16:44:45.107606-05:00"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:45.163070Z",
     "start_time": "2019-04-26T16:44:45.126161-05:00"
    }
   },
   "outputs": [],
   "source": [
    "ps = []\n",
    "ks = []\n",
    "pmfs = []\n",
    "\n",
    "for k in np.arange(1, 11, 1):\n",
    "    for p in np.linspace(0.1, .99, 10):\n",
    "        \n",
    "        p = round(p, 2)\n",
    "        ps.append(p)\n",
    "        ks.append(k)\n",
    "        pmfs.append(geom.pmf(k, p))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:45.181720Z",
     "start_time": "2019-04-26T16:44:45.166754-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'ps':ps, 'ks':ks , 'PMF': pmfs})\n",
    "\n",
    "pivoted = df.pivot('ps', 'ks', 'PMF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:46.168381Z",
     "start_time": "2019-04-26T16:44:45.186064-05:00"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(pivoted, cmap = 'magma_r', robust = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Binomial distribution\n",
    "\n",
    "* **Story.** We perform a series of Bernoulli trials until we\n",
    "get $n$ successes.  The number of failures, $k$, before we get $n$\n",
    "successes is Negative Binomially distributed.\n",
    "\n",
    "* **Parameters.** There are two parameters: the probability $p$\n",
    "of success for each Bernoulli trial, and the desired number of\n",
    "successes, $n$.\n",
    "\n",
    "* **Example.** Imagine an uber driver has to make 100 five star rides in a month order to get a bonus check. The number of $k$ non-five star rides before he/she gets the check is negatively binomially distributed. \n",
    "\n",
    "* **Probability mass function.**\n",
    "\n",
    "\\begin{align}\n",
    "f(k;n,p) = \\begin{pmatrix}\n",
    "k+n-1 \\\\\n",
    "n-1\n",
    "\\end{pmatrix}\n",
    "p^n (1-p)^k.\n",
    "\\end{align}\n",
    "\n",
    "Here, we use a combinatorial notation;\n",
    "\n",
    "\\begin{align}\n",
    "  \\begin{pmatrix}\n",
    "k+n-1 \\\\\n",
    "n-1\n",
    "\\end{pmatrix} = \\frac{(k+n-1)!}{(n-1)!\\,k!}.\n",
    "\\end{align}\n",
    "\n",
    "Generally speaking, $n$ need not be an integer, so we may write the PMF as\n",
    "\n",
    "\\begin{align}\n",
    "f(k;n, p) = \\frac{\\Gamma(k+n)}{\\Gamma(n) k!}\\,p^r(1-p)^k.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Notes.** If $n = 1$, this distribution reduces to the Geometric\n",
    "distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:49:31.444515Z",
     "start_time": "2019-04-26T16:49:30.918968-05:00"
    }
   },
   "outputs": [],
   "source": [
    "n = 8 \n",
    "p = 0.5\n",
    "ks = np.arange(0,25)\n",
    "\n",
    "ks_plot = []\n",
    "pmfs = []\n",
    "cdfs = []\n",
    "\n",
    "for i in ks:\n",
    "    \n",
    "    ks_plot.append(i)\n",
    "    pmfs.append(st.nbinom.pmf(k = i, p = p, n = n))\n",
    "    cdfs.append(st.nbinom.cdf(k = i, p = p, n = n))\n",
    "\n",
    "plt.figure(figsize = (7,8))  \n",
    "plt.subplot(2,1,1)    \n",
    "plt.plot(ks_plot, pmfs, color = 'dodgerblue', label = 'PMF')  \n",
    "plt.scatter(ks_plot, pmfs, color = 'grey')  \n",
    "plt.title('Negative binomial distribution')\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('PMF')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(ks_plot, cdfs, color = 'dodgerblue', label = 'CDF')  \n",
    "plt.scatter(ks_plot, cdfs, color = 'grey')  \n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('CDF')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:47.101894Z",
     "start_time": "2019-04-26T16:44:47.003569-05:00"
    }
   },
   "outputs": [],
   "source": [
    "##Keeping n constant\n",
    "\n",
    "n = 10 \n",
    "\n",
    "ps = np.linspace(0.2, 0.99, 20)\n",
    "\n",
    "ks = np.arange(1, 20)\n",
    "\n",
    "ps_plot = []\n",
    "\n",
    "ks_plot = []\n",
    "\n",
    "nbinom_pmfs = []\n",
    "\n",
    "for i in ps:\n",
    "    for j in ks:\n",
    "        ps_plot.append(np.round(i,2))\n",
    "        ks_plot.append(j)\n",
    "        nbinom_pmfs.append(st.nbinom.pmf(p = i, k = j, n = n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:47.887220Z",
     "start_time": "2019-04-26T16:44:47.105485-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'$p$':ps_plot, '$k$':ks_plot , 'PMF': nbinom_pmfs})\n",
    "\n",
    "pivoted_nbinom = df.pivot('$p$', '$k$', 'PMF')\n",
    "\n",
    "sns.heatmap(pivoted_nbinom, cmap = 'magma_r', robust = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:48.013829Z",
     "start_time": "2019-04-26T16:44:47.891620-05:00"
    }
   },
   "outputs": [],
   "source": [
    "##Keeping k constant\n",
    "\n",
    "k = 5\n",
    "\n",
    "ps = np.linspace(0.1, 0.99, 20)\n",
    "\n",
    "ns = np.arange(0, 20)\n",
    "\n",
    "ps_plot = []\n",
    "\n",
    "ns_plot = []\n",
    "\n",
    "nbinom_pmfs = []\n",
    "\n",
    "for i in ps:\n",
    "    for j in ns:\n",
    "        ps_plot.append(np.round(i,2))\n",
    "        ns_plot.append(j)\n",
    "        nbinom_pmfs.append(st.nbinom.pmf(p = i, k = k, n = j));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:49.023901Z",
     "start_time": "2019-04-26T16:44:48.018737-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'$p$':ps_plot, '$n$':ns_plot , 'PMF': nbinom_pmfs})\n",
    "\n",
    "pivoted_nbinom = df.pivot('$p$', '$n$', 'PMF')\n",
    "\n",
    "sns.heatmap(pivoted_nbinom, cmap = 'magma_r', robust = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial distribution\n",
    "\n",
    "* **Story.** We perform $n$ Bernoulli trials with probability $p$ of success.  The number of successes, $k$, is binomially distributed.\n",
    "\n",
    "* **Parameters.** There are two parameters: the probability $p$ of success for each Bernoulli trial, and the number of trials, $n$.\n",
    "\n",
    "* **Example.** Distribution of plasmids between daughter cells\n",
    "in cell division.  Each of the $n$ plasmids as a chance $p$ of being\n",
    "in daughter cell 1 (\"success\").  The number of plasmids, $k$, in\n",
    "daughter cell 1 is binomially distributed.\n",
    "\n",
    "* **Probability mass function.**\n",
    "\n",
    "\\begin{align}\n",
    "f(k;n,p) = \\begin{pmatrix}\n",
    "n \\\\\n",
    "k\n",
    "\\end{pmatrix}\n",
    "p^k (1-p)^{n-k}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:50.172588Z",
     "start_time": "2019-04-26T16:44:49.099441-05:00"
    }
   },
   "outputs": [],
   "source": [
    "##Keeping n constant\n",
    "\n",
    "n = 20 \n",
    "\n",
    "ps = np.linspace(0.2, 0.99, 20)\n",
    "\n",
    "ks = np.arange(1, 20)\n",
    "\n",
    "ps_plot = []\n",
    "\n",
    "ks_plot = []\n",
    "\n",
    "binom_pmfs = []\n",
    "\n",
    "for i in ps:\n",
    "    for j in ks:\n",
    "        ps_plot.append(np.round(i,2))\n",
    "        ks_plot.append(j)\n",
    "        binom_pmfs.append(st.binom.pmf(p = i, k = j, n = n))\n",
    "\n",
    "df = pd.DataFrame({'$p$':ps_plot, '$k$':ks_plot , 'PMF': binom_pmfs})\n",
    "\n",
    "pivoted_binom = df.pivot('$p$', '$k$', 'PMF')\n",
    "\n",
    "sns.heatmap(pivoted_binom, cmap = 'magma_r', robust = True);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:51.836505Z",
     "start_time": "2019-04-26T16:44:50.176464-05:00"
    }
   },
   "outputs": [],
   "source": [
    "##Keeping k constant\n",
    "\n",
    "k = 5\n",
    "\n",
    "ps = np.linspace(0.1, 0.99, 20)\n",
    "\n",
    "ns = np.arange(5, 25)\n",
    "\n",
    "ps_plot = []\n",
    "\n",
    "ns_plot = []\n",
    "\n",
    "binom_pmfs = []\n",
    "\n",
    "for i in ps:\n",
    "    for j in ns:\n",
    "        ps_plot.append(np.round(i,2))\n",
    "        ns_plot.append(j)\n",
    "        binom_pmfs.append(st.binom.pmf(p = i, k = k, n = j))\n",
    "        \n",
    "df = pd.DataFrame({'$p$':ps_plot, '$n$':ns_plot , 'PMF': binom_pmfs})\n",
    "\n",
    "pivoted_binom = df.pivot('$p$', '$n$', 'PMF')\n",
    "\n",
    "sns.heatmap(pivoted_binom, cmap = 'magma_r', robust = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson distribution\n",
    "\n",
    "* **Story.**  Rare events occur with a rate $\\lambda$ per unit\n",
    "time.  There is no \"memory\" of previous events; i.e., that rate is\n",
    "independent of time. A process that generates such events is called a *Poisson process*. The occurrence of a rare event in this context is referred to as an *arrival*. The number $k$ of arrivals in unit time is Poisson distributed.\n",
    "\n",
    "* **Parameter.** The single parameter is the rate $\\lambda$ of\n",
    "the rare events occurring.\n",
    "\n",
    "* **Example.** Neuon firing are Poisson distributed.\n",
    "\n",
    "* **Probability mass function.**\n",
    "\\begin{align}\n",
    "f(k;\\lambda) = \\frac{\\lambda^k}{k!}\\,\\mathrm{e}^{-\\lambda}.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:51.934073Z",
     "start_time": "2019-04-26T16:44:51.845747-05:00"
    }
   },
   "outputs": [],
   "source": [
    "ks = np.arange(0,21)\n",
    "mus = np.arange(1, 20)\n",
    "\n",
    "ks_plot = []\n",
    "mus_plot = []\n",
    "poisson_pmfs = []\n",
    "\n",
    "for i in ks:\n",
    "    for j in mus:\n",
    "        ks_plot.append(i)\n",
    "        mus_plot.append(j)\n",
    "        poisson = st.poisson.pmf(k = i, mu = j)\n",
    "        poisson_pmfs.append(poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:53.303662Z",
     "start_time": "2019-04-26T16:44:51.940922-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'$\\mu$':mus_plot, 'k':ks_plot , 'PMF': poisson_pmfs})\n",
    "\n",
    "pivoted = df.pivot('$\\mu$', 'k', 'PMF')\n",
    "\n",
    "sns.heatmap(pivoted, cmap= 'magma_r', robust = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypergeometric distribution\n",
    "\n",
    "* **Story.** Consider an urn with $w$ white balls and $b$ black\n",
    "balls.  Draw $n$ balls from this urn without replacement.  The number\n",
    "white balls drawn, $k$, is Hypergeometrically distributed.\n",
    "\n",
    "* **Parameters.** There are three parameters: the number of\n",
    "draws $n$, the number of white balls $w$, and the number of black\n",
    "balls $b$.\n",
    "\n",
    "* **Example.** There are $N$ finches on an island, and $n_t$ of\n",
    "them are tagged.  You capture $n$ finches.  The number of tagged\n",
    "finches $k$ is Hypergeometrically distributed, $f(k;n_t, N-n_t, n)$,\n",
    "as defined below.\n",
    "\n",
    "* **Probability mass function.**\n",
    "\n",
    "\\begin{align}\n",
    "f(k;w, b, n) = \\frac{\\begin{pmatrix}w\\\\k\\end{pmatrix}\\begin{pmatrix}b\\\\n-k\\end{pmatrix}}\n",
    "{\\begin{pmatrix}w+b\\\\n\\end{pmatrix}}.\n",
    "\\end{align}\n",
    "Alternatively, if we define $N = w + b$, we could write\n",
    "\\begin{align}\n",
    "f(k;N, w, n) = \\frac{\\begin{pmatrix}w\\\\k\\end{pmatrix}\\begin{pmatrix}N-w\\\\n-k\\end{pmatrix}}\n",
    "{\\begin{pmatrix}N\\\\n\\end{pmatrix}}.\n",
    "\\end{align}\n",
    "This is how it is [defined in the `scipy.stats` module](https://docs.scipy.org/doc/scipy-0.19.1/reference/generated/scipy.stats.hypergeom.html). In general, because distributions have different equivalent representations, it is important to check the documentation to make sure you are using the function correctly.\n",
    "\n",
    "\n",
    "* **Notes.** This distribution is analogous to the Binomial\n",
    "distribution, except that the Binomial distribution describes draws\n",
    "from an urn *with* replacement.  In the analogy, $p = w/(w+b)$.\n",
    "\n",
    "When using the sliders below, you will only get a plot if $N \\le w$ and $N \\le n$ because the distribution is only defined for these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:54.279072Z",
     "start_time": "2019-04-26T16:44:53.314435-05:00"
    }
   },
   "outputs": [],
   "source": [
    "##Keeping n and k constant, varying w and N\n",
    "\n",
    "k = 5 # white balls drawn\n",
    "n = 15 # sample size\n",
    "\n",
    "ws = np.arange(5, 20) #total number of white balls\n",
    "\n",
    "Ns = np.arange(20, 40) #total num of balls \n",
    "\n",
    "ws_plot = []\n",
    "\n",
    "Ns_plot = []\n",
    "\n",
    "hypergeom_pmfs = []\n",
    "\n",
    "for i in ws:\n",
    "    for j in Ns:\n",
    "        ws_plot.append(i)\n",
    "        Ns_plot.append(j)\n",
    "        hypergeom_pmfs.append(st.hypergeom.pmf(k = k, N = n, n = i, M = j))\n",
    "        \n",
    "df = pd.DataFrame({'$N$':Ns_plot, '$w$':ws_plot , 'PMF': hypergeom_pmfs})\n",
    "\n",
    "pivoted_hypergeom = df.pivot('$N$', '$w$', 'PMF')\n",
    "\n",
    "sns.heatmap(pivoted_hypergeom, cmap = 'magma_r', robust = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:55.524178Z",
     "start_time": "2019-04-26T16:44:54.284169-05:00"
    }
   },
   "outputs": [],
   "source": [
    "##Keeping M and N constant, varying k and n\n",
    "\n",
    "M = 40 # total no of objects\n",
    "N = 20 # sample size\n",
    "\n",
    "ns = np.arange(1, 40) # total number of white balls\n",
    "ks = np.arange(1, 20) # drawn white objects\n",
    "\n",
    "ks_plot = []\n",
    "ns_plot = []\n",
    "hypergeom_pmfs = []\n",
    "\n",
    "for i in ns:\n",
    "    for j in ks:\n",
    "        ns_plot.append(i)\n",
    "        ks_plot.append(j)\n",
    "        hypergeom_pmfs.append(st.hypergeom.pmf(k = j, N = N, n = i, M = M))\n",
    "        \n",
    "df = pd.DataFrame({'$k$':ks_plot, '$n$':ns_plot , 'PMF': hypergeom_pmfs})\n",
    "\n",
    "pivoted_hypergeom = df.pivot('$n$', '$k$', 'PMF')\n",
    "\n",
    "sns.heatmap(pivoted_hypergeom, cmap = 'magma_r', robust = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous probability distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian, a.k.a. Normal, distribution\n",
    "\n",
    "* **Story.** Any quantity that emerges as the sum of a large number of\n",
    "subprocesses tends to be Gaussian distributed provided none of the\n",
    "subprocesses is very broadly distributed.\n",
    "\n",
    "* **Parameters.** The Gaussian distribution has two parameters,\n",
    "the mean $\\mu$, which determines the location of its peak, and the\n",
    "standard deviation $\\sigma$, which is strictly positive (the\n",
    "$\\sigma\\to 0$ limit defines a Dirac delta function) and determines the\n",
    "width of the peak.\n",
    "\n",
    "* **Example.** We measure the height of the people in the room. The heights are normally distributed.\n",
    "\n",
    "* **Probability density function.**\n",
    "\n",
    "\\begin{align}\n",
    "f(x;\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\,\\mathrm{e}^{-(x-\\mu)^2/2\\sigma^2}.\n",
    "\\end{align}\n",
    "\n",
    "* **Notes.** This is a limiting distribution in the sense of the\n",
    "central limit theorem, but also in that many distributions have a\n",
    "Gaussian distribution as a limit.  This is seen by formally taking\n",
    "limits of, e.g., the Gamma, Student-t, Binomial distributions, which\n",
    "allows direct comparison of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:55.539038Z",
     "start_time": "2019-04-26T16:44:55.529322-05:00"
    }
   },
   "outputs": [],
   "source": [
    "mu = 5\n",
    "std_dev = 1.5 \n",
    "\n",
    "x = np.linspace(0, 10, 30)\n",
    "pdf = st.norm.pdf(x, loc=mu, scale= std_dev)\n",
    "cdf = st.norm.cdf(x, loc=mu, scale= std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:56.036775Z",
     "start_time": "2019-04-26T16:44:55.544239-05:00"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,8))\n",
    "plt.subplot(2,1,1)    \n",
    "plt.plot(x, pdf, color = 'dodgerblue', alpha = 0.8, label = 'PDF')\n",
    "plt.scatter(x, pdf, color = 'grey')\n",
    "plt.title('Gaussian distribution')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('PDF')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2) \n",
    "plt.plot(x, cdf, color = 'dodgerblue', alpha = 0.8, label = 'CDF')\n",
    "plt.scatter(x, cdf, color = 'grey')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('CDF')\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:56.080586Z",
     "start_time": "2019-04-26T16:44:56.041108-05:00"
    }
   },
   "outputs": [],
   "source": [
    "mus = np.arange(1, 11)\n",
    "std_dev = 1.2\n",
    "\n",
    "x = np.round(np.linspace(0, 10, 50),2)\n",
    "    \n",
    "df_plot = pd.DataFrame()\n",
    "\n",
    "for mu in mus: \n",
    "    \n",
    "    pdf = st.norm.pdf(x, loc=mu, scale= std_dev)\n",
    "    mu_list = [mu]*len(x)\n",
    "    \n",
    "    df = pd.DataFrame({'x':x, '$\\mu$':mu_list, 'pdf':pdf})\n",
    "    \n",
    "    df_plot = pd.concat([df_plot, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:57.107037Z",
     "start_time": "2019-04-26T16:44:56.083700-05:00"
    }
   },
   "outputs": [],
   "source": [
    "pivot_df = df_plot.pivot('$\\mu$', 'x', 'pdf')\n",
    "sns.heatmap(pivot_df, cmap = 'magma_r', robust = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential distribution\n",
    "\n",
    "* **Story.** This is the waiting time for an arrival from a\n",
    "Poisson process.  I.e., the inter-arrival time of a Poisson process is\n",
    "Exponentially distributed. For example, the interspike arrival time is exponentially distributed. \n",
    "\n",
    "* **Parameter.** The single parameter is the average arrival\n",
    "*rate*, $r$. Alternatively, we can use $\\tau=1/r$ as the parameter, in this case a characteristic arrival *time*.\n",
    "\n",
    "**Example.** The time between conformational switches in a\n",
    "protein is Exponentially distributed (under simple mass action\n",
    "kinetics).\n",
    "\n",
    "**Probability density function.**\n",
    "\\begin{align}\n",
    "f(x;r) = r \\mathrm{e}^{-rx}.\n",
    "\\end{align}\n",
    "\n",
    "Alternatively, we could parametrize it as\n",
    "\n",
    "\\begin{align}\n",
    "f(x;\\tau) = \\frac{1}{\\tau}\\, \\mathrm{e}^{-x/\\tau}.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Notes.** The Exponential distribution is the continuous\n",
    "analog of the Geometric distribution.  The \"rate\" in the Exponential\n",
    "distribution is analogous to the probability of success of the\n",
    "Bernoulli trial. Note also that because they are uncorrelated, the\n",
    "amount of time between any two arrivals is independent of all other\n",
    "inter-arrival times.\n",
    "\n",
    "The implementation in the `scipy.stats` module also has a location parameter, which shifts the distribution left and right. For our purposes, you can ignore that parameter, but be aware that `scipy.stats` requires it.\n",
    "\n",
    "In the `scipy.stats` implementation scale = $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:57.121368Z",
     "start_time": "2019-04-26T16:44:57.112081-05:00"
    }
   },
   "outputs": [],
   "source": [
    "tau = 3\n",
    "\n",
    "x = np.linspace(0, 10, 30)\n",
    "pdf = st.expon.pdf(x, scale = tau)\n",
    "cdf = st.expon.cdf(x, scale= tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:57.582554Z",
     "start_time": "2019-04-26T16:44:57.126090-05:00"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,8))\n",
    "plt.subplot(2,1,1)    \n",
    "plt.plot(x, pdf, color = 'dodgerblue', alpha = 0.8, label = 'PDF')\n",
    "plt.scatter(x, pdf, color = 'grey')\n",
    "plt.title('Exponential distribution')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('PDF')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)    \n",
    "plt.scatter(x, cdf, color = 'grey')\n",
    "plt.plot(x, cdf, color = 'dodgerblue', alpha = 0.8, label = 'PDF')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('CDF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:57.615257Z",
     "start_time": "2019-04-26T16:44:57.586762-05:00"
    }
   },
   "outputs": [],
   "source": [
    "taus = np.arange(2, 5, 0.5)\n",
    "\n",
    "x = np.round(np.linspace(0, 5, 50), 2)\n",
    "    \n",
    "df_plot = pd.DataFrame()\n",
    "\n",
    "tau_list = []\n",
    "\n",
    "for tau in taus: \n",
    "    \n",
    "    pdf = st.expon.pdf(x, scale= tau)\n",
    "    tau_list = [tau]*len(x)\n",
    "    \n",
    "    df = pd.DataFrame({'x':x, 'tau': tau_list, 'pdf':pdf})\n",
    "    \n",
    "    df_plot = pd.concat([df_plot, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:57.661269Z",
     "start_time": "2019-04-26T16:44:57.651059-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df_pivot = df_plot.pivot('tau', 'x', 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T21:44:58.447752Z",
     "start_time": "2019-04-26T16:44:57.664948-05:00"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df_pivot, cmap= 'magma_r', robust = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
